# 2. 上下层的抽象化

在本章节中我们将探索抽象层次的概念，其中包括了解一些分布式系统中的不可能原理（例如CAP，FLP）,最后我们又将因为性能的原因回溯，寻找其他的解决方案。

如果你接触过编程，那么抽象这个概念对你来说应该非常熟悉。我们总是在工作中接触到不同程度的抽象概念，通过对一些底层API提供的接口进行交互，从而可以为我们的用户提供更高级的API或用户界面。其中[OSI七层网络模型](http://en.wikipedia.org/wiki/OSI_model)就是一个很好的例子。

分布式程序也是如此，我敢断言，抽象在分布式系统的很大部分的处理中起了重要作用。也就是说，我们存在许多结点的现实与我们渴望系统“如同单机系统一样”两者之间有着不可避免的矛盾。为此，我们要找到一种好的抽象，来平衡哪些是可以理解的，哪些是用来表现的。

当我们说X比Y更抽象的时候，这其实意味着什么？首先，这说明X并不会添加任何新的或根本不同于Y的东西。相反，X可能还移除了一些Y的东西，或者以使它们更易于管理的方式呈现它们。其次，从某种意义上说，X比Y更易于理解，同时假设X从Y中移除的这些东西对于我们手头的事情并不重要。

[尼采](http://oregonstate.edu/instruct/phl201/modules/Philosophers/Nietzsche/Truth_and_Lie_in_an_Extra-Moral_Sense.htm) 曾经写过:

> 每一个概念其实都来自于我们将不同的事物泛化之后的共性。例如没有任意两片叶子是完全相同的，但“叶子”这个本身的概念是我们通过忽略这些具体的个体差异而抽象形成的。因此产生了这样一种观念，即有一些东西使之成为“叶子”——某些原始本质形式。但这些本质又仿佛被一双拙劣的手经过编织、标记、复制、着色、卷曲和绘制将它变得面目全非，继而导致我们无法看清其原始的本质形式。

从根本上来说，抽象是一种虚假的东西。每一种情况都是独一无二的，就像每个节点一样。但抽象使这个世界易于管理：更简单的问题描述——摒弃现实——使之更易于分析处理，而且只要我们没有忽略掉任何重要的东西，那么这个解决方案就能够广泛适用。

事实上，如果我们保留的东西是必不可少的，那么我们可以推导出的结果将会广泛适用。这就是为什么不可能原理如此重要：因为它们尽可能简单地表达一个本质上的问题，并证明在某些约束或假设下解决该问题是不可能的。

所有的抽象都忽略了某些东西，从而把现实中那些独特的事物等同起来。这其中的关键就是要去除一切不必要的东西。你怎么知道什么是必要的？嗯，你可能事先不知道。

每次我们从系统规范中排除系统的某个方面时，我们都有可能引入某些错误或性能问题。这就是为什么有时候我们需要朝另一个方向前进，并有选择地引入真实硬件和现实世界问题的某些方面回来。它可能足以重新引入一些特定的硬件特性(如物理顺序性)或其他物理特性，以获得性能良好的系统。


考虑到这一点，在使用仍可识别为分布式系统的内容时，我们应该保持的最少的现实量是什么?其中系统模型是我们认为重要的特性的规范；指定了一个系统模型之后，我们就可以了解不可能原理及其带来的挑战。

## 系统模型

顾名思义，分布式系统的关键属性是分布式，更具体地说，程序在分布式系统中：

- 在独立节点上同时运行 ...
- 通过网络连接，可能会引入非确定性和消息丢失的问题 ...
- 并且不能共享内存和时钟

这导致了很多影响：

- 每个节点并发执行一个程序
- 信息是局部的：节点只能快速访问其本地状态，而对于任何关于全局状态的信息都有可能已经过期
- 节点都会独立发生故障并从故障中恢复
- 消息可能会延迟或丢失（这与节点故障无关；同时区分网络故障和节点故障并不容易）。
- 并且时钟在节点之间不同步（本地时间戳与全局时序不符，而这点无法轻易观察到）

系统模型列举了与特定系统设计相关的许多假设。

> 系统模型
>
> 关于实施分布式系统的环境和设施的一组本质属性

系统模型对环境和设施的假设有所不同。 这些假设包括：

- 节点具有什么功能以及它们如何失败
- 通信链路如何运作，如何失效，以及
  整个系统的特性，例如时间和顺序的假设

一个健壮的系统模型应当采用尽可能弱化的假设：为这样的系统编写的任何算法对不同环境的容忍度都是非常高的，因为它做出的假设非常少。

但另一方面，我们也可以通过强有力的假设来创建一个易于推理的系统模型。例如，我们可以假设节点不会失效，意味着我们的算法不需要处理节点故障的情况。然而这样的系统模型显然是不现实的，因此很难应用到实践中。

让我们更详细地看一下系统模型中主要的三个属性的假设，包括节点、链路与时序。

### 系统模型中的节点

节点作为计算和存储的主机。它据有：

- 执行程序的能力

- 能够将数据存储到易失性内存（故障时可能丢失）和稳定状态（故障后可以读取）的能力

- 时钟（可能正确也可能不正确）

等特性

每个系统模型中的节点将执行确定的算法：发送消息和本地状态，而这将由其接受到的消息与接收时的本地状态通过算法计算而唯一确定。

有许多可能的失败模型，这些模型描述了节点失败的方式。 实际上，大多数系统都采用了崩溃-恢复故障模型，即节点只能通过崩溃来失败，并且（可能）可以在崩溃后的某个时刻恢复。

另一种选择是假设节点可以通过任意方式的错误行为而失败。这被称为[拜占庭式容错](http://en.wikipedia.org/wiki/Byzantine_fault_tolerance)。在现实世界的商业系统中，拜占庭式故障很少被处理，因为对任意故障具有弹性的算法运行成本会更高，同时实现起来也更复杂。我们将不会在这里讨论它们。

### 系统模型中的通信链路

通信链路将各个节点相互连接起来，并允许消息向任何一个方向发送。在许多讨论分布式算法的书籍中都假设每一对节点之间都有一条单独的链路，该链路提供FIFO(先进先出)的顺序传递消息，它们只能传递已发送的消息，而且发送的消息可能会丢失。

一些算法假设网络是可靠的：消息永远不会丢失，也不会无限期地延迟。对于某些现实世界的环境来说，这可能是一个合理的假设，但一般来说，最好认为网络是不可靠的，会发生信息丢失和延迟。

当网络发生故障，而节点本身保持正常运行时，我们称之为网络分区（Network Partition）。当发生这种情况时，消息可能会丢失或延迟，直到网络分区被修复。被分区的节点可能会被一些客户端访问，因此其必须与崩溃的节点区别对待。下图说明了节点故障与网络分区的关系。

![replication](http://book.mixu.net/distsys/images/system-of-2.png)

我们很少对通信链路做进一步的假设。我们可以假设链路只在一个方向上工作，或者我们可以为不同的链路引入不同的通信成本（如物理距离造成的延迟）。然而，除了长距离链路（WAN延迟）外，这些在商业环境中很少考虑这些问题，因此我在此不再赘述；而更详细的成本和拓扑模型我们也可以以复杂度为代价来获取更好的优化。

### 时序

物理分布的后果之一是，每个节点都是独特的。这是不可避免的，因为信息都是以光速传播的。如果节点彼此之间的距离不同，则从一个节点发送到另一个节点的任何消息将在不同的时间到达，并且可能以不同的顺序到达其他节点。

时序假设是获取关于我们在多大程度上考虑这一现实情况的假设的捷径。两个主要的备选方案是：

- 同步系统模型

  进程按锁步模式执行；消息传输延迟有一个已知的上限；每个进程都有一个准确的时钟。

- 异步系统模型

  无时序假设--例如，进程以独立的速率执行；消息传输延迟没有限制；不存在有用的时钟。

同步系统模型对时间和顺序添加了许多限制。它假定节点基本上具有相同的结果：发送的消息总是在特定的最大传输延迟内被接收，进程以锁步的方式执行。这很方便，因为它允许你作为系统设计者可以对时间和顺序进行假设，而异步系统模型则不然。

异步性是一个非假设：它只是假设你不能依赖时间（或 "时间传感器"）。

在同步系统模型中解决问题是相对比较容易的，因为关于执行速度、最大消息传输延迟和时钟精度等的假设都有助于解决问题，因为你可以基于这些假设进行推断，并通过假设它们从未发生来排除某些复杂而困难的故障情景。

当然，假设同步系统模型是不现实的。 现实世界中的网络容易发生故障，消息延迟也没有硬性限制。 现实世界的系统充其量只能是部分同步：它们有时可能会正常工作并提供一些消息延迟的上限，但有时消息会无限期地延迟并且时钟不同步。 我在这里不会真正讨论同步系统的算法，但是您可能会在许多其他入门书中遇到它们，因为它们在分析上比较容易（但也不切实际）。

### 共识问题

在本文的其余部分，我们将改变系统模型的参数。接下来，我们将看看如何改变两个系统属性。

- 故障模型中是否包括网络分区
- 同步与异步的时序假设。

系统设计的选择受两个不可能原理（FLP和CAP）所影响

当然，为了进行讨论，我们还需要介绍一个待解决的问题。 即[共识问题](http://en.wikipedia.org/wiki/Consensus_%28computer_science%29).

如果几台计算机（或节点）都对某个值达成一致，就能达成共识。更正式地说：

1. 协议：每一个有效的的进程都必须对同一个值达成一致。
2. 完整性：每一个进程最多只能决定一个值，如果对某个值达成一致，那么这个值一定是由某个进程提出的。
3. 终止：所有进程最终都会作出决定。
4. 有效性：如果所有进程都提出了相同的值V，那么所有进程都对V达成一致。.

共识问题是许多商用分布式系统的核心。毕竟，我们只想要分布式系统提供的性能及可靠性，而不希望处理分布式所带来的后果（比如节点之间的分歧/差异），解决了共识问题，就有可能解决一些相关的、更高级的问题，比如原子广播和原子提交。

### 两个不可能原理

第一个不可能原理，称为FLP不可能原理，是一个与设计分布式算法的人特别相关的不可能原理。第二个CAP定理与从业者更相关；即需要在不同的系统设计中进行选择，但不又与具体算法设计不直接相关的人。

## FLP

我这边简单总结一下 [FLP](http://en.wikipedia.org/wiki/Consensus_%28computer_science%29#Solvability_results_for_some_agreement_problems), 虽然它在学术界被认为是 [更重要](http://en.wikipedia.org/wiki/Dijkstra_Prize)。 FLP不可能性原理（以作者Fischer、Lynch和Patterson的名字命名）研究的是在异步系统模型下的共识问题（技术上说是协议问题，是共识问题的一种非常弱的形式）。假设节点只能通过崩溃失败；网络是可靠的，异步系统模型的典型时序假设成立：例如，消息延迟没有限制。

在这些假设下，FLP结果指出："在一个异步系统中，不存在一种（确定性的）算法来解决受故障影响所带来的共识问题，即使消息永远不会丢失，同时最多只有一个进程且它只能通过崩溃（停止执行）来失败"。

这个结果意味着，在一个非常小的系统模型下，没有办法以一种永远延迟的方式来解决共识问题。其论点是，如果存在这样一种算法，那么人们可以设计该算法的执行方式，通过延迟消息传递，使其在任意时间内保持不确定状态（"二价态"）——这在异步系统模型中是允许的。因此，这样的算法是不可能存在的。

这个不可能性原理很重要，因为它强调了假设异步系统模型会导致这样一种权衡：当关于消息传递限制得不到保证时，解决共识问题的算法必须放弃安全性或活跃性。

这个见解对设计算法的人特别有意义，因为它对我们在异步系统模型中可以解决的问题施加了严格的约束。CAP定理是一个与从业者更相关的定理：它所做的假设稍有不同（网络故障而不是节点故障），对从业者在系统设计之间的进行选择有更明确的意义。

## CAP

CAP定理最初是由计算机科学家Eric Brewer提出的一个猜想。在系统设计中考虑折衷是一种流行且相当有用的方法。 其有一个[证明](https://www.google.com/search?q=Brewer%27s+conjecture+and+the+feasibility+of+consistent%2C+available%2C+partition-tolerant+web+services)由 [Gilbert](http://www.comp.nus.edu.sg/~gilbert/biblio.html) and [Lynch](http://en.wikipedia.org/wiki/Nancy_Lynch) 提供。[Nathan Marz](http://nathanmarz.com/)并没有驳斥它，但[某讨论网站](http://news.ycombinator.com/)是这么认为的。

该定理说明了三个属性：

- 一致性(Consistency)：所有节点在同一时间看到相同的数据。
- 可用性(Availability)：节点故障不会妨碍其他正常节点继续运行。
- 分区容错性(Partition tolerance)：尽管由于网络和/或节点故障导致信息丢失，系统仍能继续运行。

以上三个属性同时只能满足两个属性。 我们甚至可以将其绘制成漂亮的图，从三个属性中选择两个属性，可以得到对应于不同交点的三种类型的系统：

![CAP theorem](http://book.mixu.net/distsys/images/CAP.png)

值得注意的是，该定理指出，中间那块（同时具备三种性质）是无法实现的。因此我们会得到三种不同类型的系统。

- CA (一致性 + 可用性)：使用包括完全严格的仲裁协议，例如两阶段提交。
- CP (一致性 + 分区容错性)： 在少数节点分区不可用的情况下，多数节点仲裁协议，如Paxos。
- AP (可用性+ 分区容错性)：使用冲突解决方案的仲裁协议，例如Dynamo。

CA和CP系统设计都提供了相同的一致性模型：强一致性。唯一不同的是，CA系统不能容忍任何节点故障；CP系统在非拜占庭故障模型中，在给定的`2f+1`个节点的情况下，最多可以容忍`f`个节点发生故障（换句话说，只要大多数`f+1`个节点保持不变，它就可以容忍少数`f`个节点的故障）。原因很简单：

- CA系统无法区分节点故障还是网络故障，因此必须停止在所有地方接受写入，以避免引入差异（多个副本）。 它无法判断是远程节点宕机，还是仅仅是网络连接断路：所以唯一安全的做法是停止接受写入。
- CP系统通过在分区的两侧强制非对称行为来防止差异（例如，保持单副本一致性）。它仅保留多数分区正常运行，而要求少数分区变得不可用（如停止接受写入），这样既保留了一定的可用性（多数分区），又能保证单副本的一致性。

关于Paxos算法时，我将在副本一章中更详细地讨论这个问题。重要的是，CP系统将网络分区纳入其故障模型中，并使用Paxos、Raft或带有视图的复制等算法区分多数分区和少数分区。CA系统没有分区意识，历史上比较常见：它们通常使用两阶段提交算法，并且在传统的分布式关系数据库中很常见。

假设发生了分区，该定理简化为可用性和一致性之间的二元选择。

![Based on http://blog.mikiobraun.de/2013/03/misconceptions-about-cap-theorem.html](http://book.mixu.net/distsys/images/CAP_choice.png)

我认为从CAP定理中应该可以得出四个结论。

首先，*早期的分布式关系数据库系统中使用的许多系统设计都没有考虑到分区容忍度*（例如它们是CA设计）。对于现代系统来说，分区容错性是一个重要的属性，因为如果系统在地理上是分布式的(许多大型系统都是这样)，网络分区的可能性就会变得更大。

其次，*在网络分区期间，强一致性和高可用性之间存在矛盾。*CAP定理说明了强保证和分布式计算之间的权衡。

从某种意义上说，承诺一个由不可预测的网络连接的独立节点所组成的分布式系统 "使其行为方式与非分布式系统完全一样 "是相当疯狂的。

![From the Simpsons episode Trash of the Titans](http://book.mixu.net/distsys/images/news_120.jpg)

强一致性保证要求我们在分区期间放弃可用性。这是因为人们无法防止两个无法相互通信的副本之间的分歧，同时继续接受分区两边的写入。

我们如何解决这个问题？通过加强假设（假设没有分区）或削弱要求。通过权衡一致性和可用性（以及离线可访问性和低延迟的相关能力）。如果 "一致性 "被降低要求定义为"所有节点在同一时间看到相同的数据"，那么我们可以同时拥有可用性和一些（较弱的）一致性保证。

第三，*在正常操作中，强一致性和性能之间存在矛盾。*

强一致性/单副本一致性要求节点在每次操作时都要进行通信并达成一致。这就导致了正常操作时的高延迟。

如果您可以接受经典模型以外的一致性模型（一种允许副本滞后或有差异的一致性模型），那么您可以减少正常操作期间的延迟并在存在分区的情况下保持可用性。

当涉及较少的消息和较少的节点时，操作可以更快地完成。但实现这一目标的唯一方法是放宽要求：让部分节点的联系频率降低，这意味着节点可以包含旧数据。

这也使得异常情况的发生成为可能。你不再保证能得到最新的数据。根据什么样的要求，你可能会读到一个比预期更早的值，甚至失去一些更新。

第四--间接的表明，*如果我们不想在网络分区期间放弃可用性，那么我们需要探索除强一致性以外的一致性模型对我们的目的是否可行*。

例如，即使用户数据被复制到多个数据中心，而这两个数据中心之间的链接暂时失灵，在很多情况下，我们仍然希望允许用户使用网站/服务。这意味着以后要调和两组不同的数据，这既是技术上的挑战，也是业务上的风险。但往往技术挑战和业务风险都是可控的，因此最好是提供高可用性。

一致性和可用性其实并不是二元选择，除非你把自己限制在强一致性上。但是，强一致性只是一种一致性模型：在这种模型中，为了防止数据的多个副本处于活动状态，你必然需要放弃可用性。正如[Brewer自己指出的](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed)三选二的解释是具有误导性的。

如果你从这次讨论中只得到一个想法，那就是： "一致性 "并不是一个单一的，明确的属性。请记住：

> [ACID](http://en.wikipedia.org/wiki/ACID) 的一致性!= 
> [CAP](http://en.wikipedia.org/wiki/CAP_theorem) 的一致性!= 
> [燕麦片](http://en.wikipedia.org/wiki/Oatmeal) 的稠度

相反，一致性模型是数据存储区对给使用它的程序的一种保证

- 一致性模型

  程序员和系统之间的合同，其中系统保证，如果程序员遵循一些特定的规则，对数据存储区的操作结果将是可预测的。

CAP中的 "C "是 "强一致性"，但 "一致性 "不是 "强一致性 "的同义词。

我们来看看一些可供选择的一致性模型。

## 强一致性与其他一致性模型的比较

一致性模型可以分为两类：强一致性模型和弱一致性模型。

- 强一致性模型(能够维护单一副本)
  - 线性一致性
  - 顺序一致性
- 弱一致性模型
  - 以客户端为中心的一致性模型
  - 因果一致性：可用的最强模式
  - 最终一致性模型

强一致性模型保证更新的表观顺序和可见性等同于一个非复制的系统。而弱一致性模型则不做这种保证。

注意，这绝不是一个详尽的清单。同样，一致性模型只是程序员和系统之间的契约，所以它们几乎可以是任何东西。

### 强一致性模型

强一致性模型可以进一步分为两个相似但略有不同的一致性模型：

- *线性一致性*。在线性一致性情况下，所有的操作**看起来**都和全局时钟下的顺序一致。(Herlihy & Wing, 1991)
- *顺序一致*。在顺序一致的情况下，所有操作**看起来**是按某种顺序原子地执行的，这种顺序与各个节点的顺序一致，而且在所有节点上都是平等的。(Lamport，1979)

关键的区别在于，线性一致性要求操作生效的顺序等于操作的实际实时顺序。顺序一致性允许操作重新排序，只要每个节点上观察到的顺序保持一致即可。能够区分这两者的唯一方法是，如果他们能够观察到所有进入系统的输入和时序；从客户端与节点交互的角度来看，两者是等价的。

区别似乎无关紧要，但值得注意的是，顺序一致并没有构成。

强一致性模型让你作为一个程序员，可以用分布式节点的集群来代替单台服务器，而不会遇到任何问题。

所有其他的一致性模型都有异常情况（与保证强一致性的系统相比），因为它们的行为方式与非复制系统是有区别的。但往往这些异常是可以接受的，要么是因为我们不在乎偶发问题，要么是因为我们在不一致发生后以某种方式编写了处理不一致的代码。

请注意，对于弱一致性模型，实际上并没有任何通用类型，因为“不是强一致性模型”（例如，“在某种程度上可与非复制系统区分开”）几乎可以是任何东西。

### 以客户端为中心的一致性模型

*以客户端为中心的一致性模型*是以某种方式涉及客户端或会话概念的一致性模型。例如，以客户端为中心的一致性模型可以保证客户端永远不会看到数据项的旧版本。这通常是通过在客户端库中构建额外的缓存来实现的，因此，如果客户端移动到包含旧数据的副本节点，那么客户端库就会返回其缓存值，而不是来自副本的旧值。

如果客户端所在的副本节点不包含最新版本，客户端可能仍然会看到旧版本的数据，但他们永远不会看到旧版本值重新出现的异常情况（例如，因为他们连接到了不同的副本节点）。注意，以客户端为中心的一致性模型有很多种。

### 最终一致性模型

*最终一致性*模型说，如果你停止改变数值，那么在经过一段不确定的时间之后，所有的副本将在相同的数值上达成一致。这意味着在该时间之前，副本之间的结果以某种未定义的方式不一致。由于它是[可轻易满足的](http://www.bailis.org/blog/safety-and-liveness-eventual-consistency-is-not-safe/) (仅指活跃属性)，因此没有补充信息是无用的。

说某件事情仅仅是最终一致，就像说 "人最终是死的 "一样。这是一个非常弱的约束，我们可能希望至少对两件事有一些更具体的描述：

首先，"最终 "是多长时间？如果能有一个严格的下限，或者至少对系统收敛到相同值通常需要多长时间有一些了解，那将是有益的。

其次，副本如何就一个值达成一致？一个总是返回 "42 "的系统最终是一致的：所有的副本都在同一个值上达成一致。只是它不会收敛到一个有用的值，因为它只是一直返回相同的固定值。相反，我们希望对这个方法有一个更好的想法。例如，一种决定的方法是始终获得最大时间戳记的值。

所以当供应商说 "最终一致性 "的时候，他们的意思是一些更精确的术语，比如 "最终最后一个写入者获胜，在此期间读取最新观测值 "的一致性。"怎么做？"很重要，因为一个不好的方法可能会导致写入丢失--例如，如果一个节点的时钟设置错误，并且使用了时间戳。

我将在弱一致性模型的复制方法一章中更详细地研究这两个问题。

------

## 扩展阅读

- [Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services](http://lpd.epfl.ch/sgilbert/pubs/BrewersConjecture-SigAct.pdf) - Gilbert & Lynch, 2002
- [Impossibility of distributed consensus with one faulty process](http://scholar.google.com/scholar?q=Impossibility+of+distributed+consensus+with+one+faulty+process) - Fischer, Lynch and Patterson, 1985
- [Perspectives on the CAP Theorem](http://scholar.google.com/scholar?q=Perspectives+on+the+CAP+Theorem) - Gilbert & Lynch, 2012
- [CAP Twelve Years Later: How the "Rules" Have Changed](http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed) - Brewer, 2012
- [Uniform consensus is harder than consensus](http://scholar.google.com/scholar?q=Uniform+consensus+is+harder+than+consensus) - Charron-Bost & Schiper, 2000
- [Replicated Data Consistency Explained Through Baseball](http://pages.cs.wisc.edu/~remzi/Classes/739/Papers/Bart/ConsistencyAndBaseballReport.pdf) - Terry, 2011
- [Life Beyond Distributed Transactions: an Apostate's Opinion](http://scholar.google.com/scholar?q=Life+Beyond+Distributed+Transactions%3A+an+Apostate%27s+Opinion) - Helland, 2007
- [If you have too much data, then 'good enough' is good enough](http://dl.acm.org/citation.cfm?id=1953140) - Helland, 2011
- [Building on Quicksand](http://scholar.google.com/scholar?q=Building+on+Quicksand) - Helland & Campbell, 2009