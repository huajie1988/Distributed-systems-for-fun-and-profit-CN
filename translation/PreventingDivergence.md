# 4. 复制

复制问题是分布式系统中众多问题中的一个。我之所以选择把重点放在它上，而不是其他问题，如领导者选举、失效检测、互斥、共识和全局快照，是因为它往往是人们最感兴趣的部分。例如，并行数据库的一个区分方式是在其复制功能方面。此外，复制还为许多子问题提供了背景，例如领导者选举、失败检测、共识和原子广播。

复制是一个群体通信问题。什么样的通信模式能给我们带来我们想要的性能和可用性？面对网络分区、节点同时失效的情况，我们如何保证容错性、持久性、不发散性？

这里我将采用的方法只关注复制系统可能出现的高级模式。以可视化的方式来看待这个问题，有助于将讨论的重点放在总体模式上，而不是涉及到具体的消息传递。我在这里的目标是探索设计空间，而不是解释每个算法的细节。

首先我们来定义一下复制是什么样的。我们假设我们有一些初始的数据库，客户端发出的请求会改变数据库的状态。

![replication](http://book.mixu.net/distsys/images/replication-both.png)

然后，可以将通信模式分为几个阶段：

1. (Request) 客户端向服务器发送请求
2. (Sync) 复制的同步部分发生
3. (Response) 将响应返回给客户端
4. (Async) 复制的异步部分发生

这个模型是基于[这篇文章](https://www.google.com/search?q=understanding+replication+in+databases+and+distributed+systems)简易构建的。需要注意的是，每部分任务中交换消息的方式取决于具体算法。我有意不讨论具体的算法。

鉴于这些阶段，我们可以创建什么样的通信模式？我们选择的方式又会对性能和可用性产生什么影响？

## 同步复制

第一种模式是同步复制（也叫主动复制，或急切复制，或推送复制，或悲观复制）。我们来画一下是什么样子的：

![replication](http://book.mixu.net/distsys/images/replication-sync.png)

在这里，我们可以看到三个不同的阶段：首先，客户端发送请求。接下来，我们称之为复制的同步部分。这个术语指的是客户端被阻塞--等待系统的回复。

在同步阶段，第一台服务器与另外两台服务器联系，直到收到所有其他服务器的答复。最后，它向客户机发送响应，告知结果（如成功或失败）。

这一切似乎是直截了当的。如果不讨论同步阶段的算法细节，我们能对这种具体的通信模式说些什么呢？首先，观察一下，这是一种N个节点中N个节点都必须写入的方式：在返回一个响应之前，它必须被系统中的每一个服务器看到并确认。

从性能的角度来看，这意味着系统将和其中最慢的服务器一样快。该系统对网络延迟的变化也将非常敏感，因为它要求每个服务器在继续之前都要进行答复。

鉴于N-of-N的方式，系统不能容忍任何服务器的丢失。当服务器丢失时，系统无法再对所有节点进行写入，因此无法继续进行。它也许能够提供对数据的只读访问，但在这种设计中，在节点失效后将不允许修改。

这种配置可以提供很强的持久性保障：当响应返回时，客户端可以确定所有N个服务器都已经接收、存储和确认了请求。当某台服务器丢失一个已被接受的更新时，那么其他所有的N个副本也都需要丢失该更新，这是你能做的最好的保障。

## 异步复制

让我们将其与第二种模式--异步复制（也就是被动复制，或拉取复制，或惰性复制）进行对比。正如你可能已经猜到的，它与同步复制完全相反。

![replication](http://book.mixu.net/distsys/images/replication-async.png)

在这里，master服务器（/领导者/协调者）会立即向客户端发送回一个响应。它可能最多只能将更新存储在本地，而不会同步执行任何其他重要的工作，同时客户端也无需被迫等待各服务器之间发生的多轮的通信。

在稍后的某个阶段，复制任务的异步部分会发生。在这里，master服务器使用某种通信模式联系其他服务器，其他服务器则更新它们的数据副本。具体内容取决于所使用的算法。

在不涉及算法细节的情况下，我们能对这种具体配置说些什么呢？好吧，这是一种1-of-N的方式：立即返回一个响应，同时更新传播将在以后的某个时间发生。

从性能的角度来看，这意味着系统的速度很快：客户端不需要花费任何额外的时间来等待系统内部的工作。该系统对网络延迟的容忍度也更高，因为内部延迟的波动不会导致客户端的额外等待。

这种安排只能提供弱的，或者说是概率性的持久性保障。如果没有出现任何问题，数据最终会被复制到所有N台机器上。然而，如果包含数据的唯一服务器在这之前丢失，数据将永久丢失。

考虑到这是1-of-N的方式，只要至少有一个节点启动，系统就可以保持可用（至少在理论上是这样，尽管在实践中负载可能会过高）。像这种纯粹的惰性方法没有提供任何持久性或一致性的保证；你可能被允许向系统写东西，但不能保证在发生任何故障时你能读回你写的东西。

最后，值得注意的是，被动复制无法确保系统中的所有节点始终包含相同的状态。如果你在多个位置接受写入，并且不要求这些节点同步同意，那么你将会面临数据不一致的风险：读取的数据可能会从不同的位置返回不同的结果（特别是在节点失败和恢复之后），并且全局约束（需要与每台主机进行通信）无法执行。

我还没有真正提到读取（而不是写入）期间的通信模式，因为读取的模式确实遵循写入的模式：在读取期间，你希望联系尽可能少的节点。我们将在讨论Quorums协议时再讨论一下这个问题。

我们只讨论了两种基础的通信模式，没有讨论具体的算法。但是，我们足以能够弄清可能的通信模式及其性能、持久性保证和可用性。

## 主流复制方法概述

在讨论了同步复制和异步复制这两种基本的复制方式之后，让我们再来看看主流的复制算法。

有许多不同的方法可以对复制技术进行分类。我想介绍的第二种分类依据（继同步与异步之后）是：

- 防止数据差异的复制方法(单拷贝系统)
- 有可能出现数据差异的复制方法（多master系统）

第一组方法具有"行为像单一系统"的特性。特别是，当发生部分故障时，系统确保只有系统的一个副本处于活动状态。此外，系统还确保副本始终保持一致。这就是所谓的共识问题。

如果几个进程（或计算机）都达成一致，就能达成共识。更正式地说：

1. 协议：每一个有效的进程都必须对同一个值达成一致。
2. 完整性：每一个进程最多只能决定一个值，如果对某个值达成一致，那么这个值一定是由某个进程提出的。
3. 终止：所有进程最终都会作出决定。
4. 有效性：如果所有进程都提出了相同的值V，那么所有进程都对V达成一致。.

互斥、领导者选举、多播和原子广播都是共识这一更普遍问题的实例。维持单副本一致性的复制系统需要以某种方式解决共识问题。

维持单副本一致性的复制算法包括：

- 1n messages (异步主从备份)
- 2n messages (同步主从备份)
- 4n messages (两阶段提交, Multi-Paxos)
- 6n messages (三阶段提交、重选领导者的Paxos)

这些算法在其容错性上有所不同（例如它们能容忍的故障类型）。我简单地按照算法执行过程中交换的消息数量对这些算法进行了分类，因为我认为，试图找到"我们用增加的消息交换买到了什么？"这个问题的答案是很有意思的。

下图改编自[Google](http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html)的Ryan Barret，描述了不同方案的一些方面：

![Comparison of replication methods, from http://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html](http://book.mixu.net/distsys/images/google-transact09.png)

上图中的一致性、延迟、吞吐量、数据丢失和故障切换等特性，其实可以追溯到两种不同的复制方式：同步复制（如等待再响应）和异步复制。当你等待时，你会得到更差的性能，但更强的保障。当我们讨论网络分区(和延迟)容限时，2PC和Quorum系统之间的吞吐量差异将变得明显。

在该图中，弱（最终）一致性的算法被归为一类（"Gossip"）。然而，我将更详细地讨论弱一致性的复制方法--Gossip和Partial Quorum系统。"事务"一行其实更多的是指全局评估，在弱一致性的系统中不支持全局评估（虽然可以支持局部评估）。

值得注意的是，执行弱一致性要求的系统具有较少的通用算法，并且可以有选择地应用更多技术。由于系统不强制执行单副本一致性，因此可以像由多个节点组成的分布式系统一样自由运行，并很少需要修正，而重点更多地在于为人们提供一种推理方法，以了解他们拥有的系统特性。

例如：

- 以客户端为中心的一致性模型试图在允许数据差异的同时提供更容易理解的一致性保证。
- CRDTs（convergent and commutative replicated datatypes）利用了基于状态的semilattice属性（结合律、交换律、幂等性）和基于操作的数据类型。
- 汇总分析（如Bloom语言）利用有关计算单调性的信息，最大限度地利用无序性。
- PBS(probabilistically bounded staleness)通过使用模拟和从真实世界系统中收集的信息来描述Partial Quorum系统的预期行为。

这些我都会讲得比较深入，先来看看单副本一致性的复制算法。

## 主从备份

主从备份（也称为主副本复制、主从复制或日志传输）可能是最常用的复制方法，也是最基本的算法。所有的更新都是在主服务器（master）上进行的，而操作的日志（或者说是变更）则通过网络运到备份服务器上生成数据副本。其有两种变体。

- 异步主从备份
- 同步主从备份

同步版本需要两个消息（"更新"+"确认收到"），而异步版本只需要一个消息（"更新"）就可以运行。

P/B是非常常见的。例如，默认情况下，MySQL复制使用异步方式。MongoDB也使用P/B（有一些额外的程序用于故障转移）。所有的操作都在一个主服务器上进行，主服务器将它们序列化到本地日志，然后异步复制到备份服务器。

正如我们前面在异步复制的背景下所讨论的，任何异步复制算法都只能提供较弱的持久性保证。在MySQL复制中，这表现为复制滞后：异步备份总是比主备至少慢一个操作。如果主站失败，那么还没有发送到备份的更新就会丢失。

主从备份的同步方式可以确保在返回客户端之前，写入的数据已经存储在其他节点上--代价是等待其他备份节点的响应。然而，值得注意的是，即使是这种方式也只能提供较弱的保证。考虑以下简单的故障场景：

- 主服务器接收到写的内容，并将其发送到备份服务器。
- 备份服务器写入成功并返回ACK
- 主服务器在发送ACK给客户端之前发生故障

客户端现在假设提交失败，但备份却实际提交成功了；如果备份服务器晋升为主服务器，则会不正确。可能需要手动清理，以协调失败的主备份或数据有差异的备份。

当然，我在这里做了简化。虽然所有的主从备份复制算法都遵循相同的一般消息传递模式，但它们在处理故障迁移、备份服务器长时间离线等问题上有所不同。然而，在这个方案中，不可能对主服务器不适当的故障进行弹性处理。

在基于日志运输/主从备份的方案中，关键是它们只能提供最大努力的保证（例如，如果节点在不适当的时间失败，它们很容易丢失更新或错误的更新）。此外，P/B方案容易出现分脑现象，即由于临时的网络问题导致故障切换到备份，并导致主服务器和备份同时激活。

为了防止不恰当的故障导致一致性保证被违反；我们需要再增加一轮消息传递，这就得到了两阶段提交协议（2PC）

## 两阶段提交(2PC)

[两阶段提交](http://en.wikipedia.org/wiki/Two-phase_commit_protocol) (2PC)是许多经典关系型数据库中使用的协议。例如，MySQL Cluster（不要和普通的MySQL混淆）使用2PC提供同步复制。下图说明了消息流：

是许多经典关系型数据库中使用的协议。例如，MySQL Cluster（不要和普通的MySQL混淆）使用2PC提供同步复制。下图说明了消息流：

```
[ Coordinator ] -> OK to commit?     [ Peers ]
                <- Yes / No

[ Coordinator ] -> Commit / Rollback [ Peers ]
                <- ACK
```

在第一阶段(投票)，协调者向所有参与者发送更新信息。每个参与者处理更新，并投票决定是提交还是放弃。当投票决定提交时，参与者将更新存储到一个临时区域（写头日志）。在第二阶段完成之前，更新被视为临时性的。

在第二阶段(决策)，协调者决定结果，并通知每个参与者。如果所有参与者都投票同意提交，那么更新就从临时区域取出，并成为永久性的。

在提交被认为是永久性的之前，有一个第二阶段是有用的，因为它允许系统在一个节点失败时回滚更新。相比之下，在主从备份("1PC")中没有一个因为在一些节点上更新失败而在另一些节点上更新成功而回滚的步骤，因此备份可能会出现数据差异。

2PC很容易发生阻塞，因为单个节点故障(参与者或协调者)都会导致进度阻塞，直到该节点恢复为止。恢复通常是可能的，这要归功于第二阶段，在这一阶段，其他节点将被告知系统状态。需要注意的是，2PC假设每个节点稳定存储的数据永远不会丢失，也没有节点永远崩溃。如果稳定存储中的数据在崩溃中被破坏，仍然可能丢失数据。

节点故障时恢复程序的细节相当复杂，所以我将不再赘述。主要的任务是确保写入磁盘的数据是持久的（例如刷新到磁盘而不是缓存），并确保做出正确的恢复决策（例如了解在故障期间的结果，然后在本地重做或撤销更新）。

正如我们在关于CAP的一章中了解到的，2PC是一个CA--它不是分区容错的。2PC解决的故障模型不包括网络分区；从节点故障中恢复的规定方式是等待网络分区修复。如果一个协调者失败了，没有安全的方法来产生新的协调者，而是需要人工干预。2PC对延迟也相当敏感，因为它是一种写N-of-N的方式，在最慢的节点确认之前，写入不能进行。

2PC在性能和容错性之间取得了不错的平衡，这也是它在关系型数据库中受到欢迎的原因。然而，较新的系统通常使用分区容错共识算法，因为这种算法可以提供从临时网络分区中自动恢复的能力，以及更优雅地处理节点间延迟的增加。

接下来我们来看看分区容错共识算法

## 分区容错共识算法

分区容错共识算法是在我们的单副本一致性容错算法方面更进一步。还有另一类容错算法：容许[任意（拜占庭）故障](http://en.wikipedia.org/wiki/Byzantine_fault_tolerance)的算法；其中包括因恶意行为而失败的节点。这种算法在商业系统中很少使用，因为它们的运行成本较高，实现起来也比较复杂--因此我就不说了。

谈到分区容错共识算法，最著名的算法就是Paxos算法。然而，它是众所周知的难以理解和实现，所以我将重点介绍Raft，这是一个最近（约2013年初）设计的算法，更容易教学和实现。我们先来看看网络分区和分区容错共识算法的一般特点。

### 什么是网络分区？

网络分区是指一个或几个节点的网络链路发生故障。节点本身继续保持活跃，它们甚至可以接收到来自它们这一边的客户端的请求。正如我们在前面--在讨论CAP定理时所了解到的那样，网络分区确实会发生，而且并不是所有系统都能优雅地处理它们。

网络分区是很棘手的，因为在网络分区期间，无法区分是远程节点失败还是该节点无法到达。如果发生了网络分区，但没有节点失效，那么系统就会被分为两个同时处于活动状态的分区。下面两张图说明了网络分区与节点故障的情况。

2个节点的系统， 节点故障 vs. 网络分区：

![replication](http://book.mixu.net/distsys/images/system-of-2.png)

3个节点的系统， 节点故障 vs. 网络分区：

![replication](http://book.mixu.net/distsys/images/system-of-3.png)

一个强制执行单副本一致性的系统必须有某种方法来打破对称性：否则，它就会分裂成两个独立的系统，而这两个系统可能会相互背离，继而无法再维持单副本系统的假象。

网络分区容错性对于执行单副本一致性的系统来说，要求在网络分区期间，系统只有一个分区保持活动状态，因为在网络分区期间不可能防止数据差异（如CAP定理）。

### 多数派决策

这就是为什么分区容错共识算法依赖于多数投票的原因。要求大多数节点——而不是所有节点（如2PC）——同意更新，允许少数节点因网络分区而宕机、速度慢或无法访问。只要N个节点中的(N/2+1)个节点能启动和访问，系统就能继续运行。

分区容错共识算法使用奇数个节点（如3、5或7）。如果只有两个节点，则不可能在失败后存在明显的多数派。例如，如果节点数是三个，那么系统对一个节点故障有弹性；如果有五个节点，系统对两个节点故障有弹性。

当发生网络分区时，分区的行为是不对称的。一个分区将包含大多数节点。少数分区将在网络分区期间停止处理操作以防止数据差异，但多数分区可以保持活动状态。这确保了系统状态只有一个副本保持活跃。

多数决是有效的，因为它们可以容许分歧：如果出现扰动或失败，节点可能会有不同的投票。但是，由于只能有一个多数决定，因此暂时的分歧最多只能阻止协议的进行，但不能违反单副本一致性(安全性)。

### 角色

有两种方式可以构建一个系统：所有节点可能具有相同的职责，或者每个节点可能具有单独的、不同的角色。

复制的共识算法一般选择为每个节点设置不同的角色。拥有一个固定的领导者或主服务器是一种优化，可以让系统更有效率，因为我们知道所有的更新都必须通过该服务器。不是领导者的节点只需要将自己的请求转发给领导者即可。

请注意，拥有不同的角色并不妨碍系统从领导者（或任何其他角色）的失败中恢复。因为角色仅仅只是在正常运行期间是固定的，这并不意味着系统不能在失败后通过重新分配角色（例如通过领导者选举阶段）从失败中恢复。节点可以重复使用领导者选举的结果，直到发生节点故障或网络分区。

Paxos和Raft都使用了不同的节点角色。特别是，它们都有一个领导者*(Leader)*节点（Paxos中的"提议者*(proposer)*"），负责正常运行期间的协调。在正常运行期间，其余节点都是追随者*(Followers)*（Paxos中的"批准人*(acceptors)*"或"投票者*(voters)*"）。

### 任期

在Paxos和Raft中，每一个正常运行的时期被称为一个任期*(Epoch)*（Raft中的"term"）。在每个任期中，只有一个节点被指定为领导者（[日本采用](http://en.wikipedia.org/wiki/Japanese_era_name)类似的制度，在日本，时代名称在皇室继承后会发生变化）。

![replication](http://book.mixu.net/distsys/images/epoch.png)

在选举成功后，由该领导者负责协调，直到任期结束。如上图所示（来自Raft文件），有些选举可能会失败，导致任期立即结束。

任期值*(Epochs)*作为一个逻辑时钟，允许其他节点识别过时的节点何时开始通信——被分区或停止运行的节点将拥有比当前节点更小的任期值，所以它们的命令将被忽略。

### 竞争领导者

在正常运行过程中，分区容错共识算法相当简单。正如我们前面所看到的，如果我们不关心容错，我们可以直接使用2PC。而大部分的复杂性其实是来自于一旦达成共识决定后，其不会丢失同时协议需要可以处理网络或节点故障导致的领导者变更。

所有节点都以追随者*(Followers)*的身份开始；在开始时，有一个节点被选为领导者。在正常运行过程中，领导者会保持心跳，让追随者能够检测到领导者是否失败或被分区。

当一个节点检测到一个领导者已经变得没有反应（或者，在初始情况下，没有领导者存在），它就会切换到一个中间状态（在Raft中称为"候选人*(candidate)*"），在这个状态下，它将任期值递增1，发起领导者选举，并竞争成为新的领导者。

为了当选领导者，一个节点必须获得多数票。分配选票的一种方式是简单地按照先到先得的原则分配选票，这样一来，最终会有一个领导者当选。在尝试当选之间增加随机的等待时间，将减少同时尝试当选的节点数量。

### 一届任期内的编号提案

在每届任期，领导者每次提出一个值进行投票。在每届任期，每个提案都有一个独特的严格递增的编号。追随者(投票者/批准人)接受他们收到的关于特定提案编号的第一个提案。

### 正常操作

在正常操作期间，所有的提案都要经过领导者节点。当客户端提交提案时（如更新操作），领导者会联系法定节点数中的所有节点。如果没有竞争性的提案存在（根据追随者的响应），领导者提出某个值。如果大多数追随者接受该值，则认为该值被接受。

由于有可能另一个节点也在试图充当领导者，我们需要确保一旦有一个提案被接受，它的值永远不能改变。否则，一个已经被接受的提案可能会被竞争的领导者恢复。Lamport对此的阐述是：

> P2: 如果一个值为 `v` 的提案被选中, 那么之后每一个被选中的较高编号的提案都有这个值 `v`.

要确保这个属性成立，就要求追随者和提案人都受到算法的约束，不能改变已经被大多数节点接受的值。请注意，"该值永远不能改变"指的是协议的单次执行（或运行/实例/决定）的值。一个典型的复制算法将多次执行该算法，但为了保持简单，大多数关于算法的讨论都集中在单次运行上。我们要防止决策历史记录被更改或覆盖。

为了执行这个属性，提案人必须先向追随者询问他们（编号最高的）被接受的提案及其值。如果提案人发现已经存在一个提案，那么它必须简单地完成这次协议的执行，而不是提出自己的提案。Lamport将其表述为：

> P2b. 如果一个值为 `v` 的提案被选中, 那么，任何提案人发出的每一项较高编号的提案都有该值`v`.

更具体地说：

> P2c. 对于任意 `v` 和`n`，如果一个提案值为 `v` 同时提案号为 `n` 的提案是由[领导者]发出的,那么有一个由大多数批准人[追随者]组成的集合 `S` ，要么(a)  `S` 中没有任何接受者接受了提案号小于 `n`的提案, 要么(b) `v` 是 `S`中被追随者所接受的提案号比 `n` 小的所有提案中编号最高的那个值。

这是Paxos以及由其衍生算法的核心。直到协议的第二阶段才会选择提案值。提案人有时必须简单地重传之前作出的决策，以确保安全(如P2c中的b条款)，直到他们知道可以自由地推行自己的提案值(如a条款)。

如果先前存在多个提案，则提出编号最高的提案值。只有在没有任何竞争性提案的情况下，提案人才可以尝试推行自己的提案值。

为了确保在提案人询问各接受者最新数值这段时间不会出现竞争性提案，提案人要求追随者不要接受比当前提案号更低的提案。

综合来看，使用Paxos达成决策需要两轮沟通：

```
[ Proposer ] -> Prepare(n)                                [ Followers ]
             <- Promise(n; previous proposal number
                and previous value if accepted a
                proposal in the past)

[ Proposer ] -> AcceptRequest(n, own value or the value   [ Followers ]
                associated with the highest proposal number
                reported by the followers)
                <- Accepted(n, value)
```

在准备阶段，提案人可以了解任何竞争性的或以前的提案。第二阶段是提出一个新的提案值或一个之前已被接受的提案值。在某些情况下--例如出现两个提案人同时活跃（竞争）；如果消息丢失；或者如果大多数节点都发生了故障--那么多数节点不会接受任何提案。但这是可以接受的，因为决策规则致使提案值会收敛于一个单值（前一次尝试中提案号最高的那个）。

事实上，根据FLP定理，这是我们能做的最好的事情：当关于消息传递边界的保证失效时，解决一致问题的算法必须要么放弃安全性，要么放弃可用性。Paxos放弃了可用性：它可能不得不无限期地推迟决策，直到没有竞争的领导者，并且大多数节点都接受一个提案时。这比违反安全性更为可取。

当然，实现此算法比听起来要难得多。即使在专家手中，其中的许多小问题加起来也会产生相当可观的代码量。这些问题包括：

- 实际优化：
  - 通过领导者租约(而不是心跳)来避免反复的领导者选举
  - 在领导者身份不变的稳定状态下，避免重复的提案信息
- 确保追随者和提案人不会丢失稳定存储中的数据，确保稳定存储中的结果不会被破坏（如磁盘损坏）。
- 允许以安全的方式更改集群成员(例如基于Paxos依赖于多数节点总是相交于一个节点的事实，如果成员可以任意更改，则这个事实就不成立)
- 在发生崩溃、磁盘丢失或配置新节点后，以安全、高效的方式更新新副本的程序。
- 经过一段合理的时间后，对保证安全所需的数据进行快照和垃圾回收的程序(例如，平衡存储需求和容错需求)

谷歌的[Paxos Made Live](http://labs.google.com/papers/paxos_made_live.html)文件详细介绍了其中的一些挑战。

## 分区容错共识算法: Paxos, Raft, ZAB

希望这能让你对分区容错共识算法的工作原理有所了解。我鼓励你阅读“扩展阅读”部分中的一篇论文，以了解不同算法的具体内容。

*Paxos*. Paxos是编写强一致性分区容错复制系统时最重要的算法之一。它被用于Google的许多系统中，包括[BigTable](http://research.google.com/archive/bigtable.html)[/Megastore](http://research.google.com/pubs/pub36971.html)使用的[Chubby lock manager](http://research.google.com/archive/chubby.html)、Google文件系统以及[Spanner](http://research.google.com/archive/spanner.html)。

Paxos是以希腊的Paxos岛命名的，最初是由Leslie Lamport在1998年的一篇名为"The Part-Time Parliament"的论文中提出的。它通常被认为是难以实现的，并且已经有一系列来自于具有分布式系统专业知识的公司的论文进一步的解释了实际细节（见扩展阅读）。你可能想在[这里](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#lamport-paxos)和[这里](http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html#paxos-simple)阅读Lamport对这个问题的评论。

这些问题主要涉及到Paxos是以单轮共识决策来描述的，但实际工作中的实现通常希望高效地运行多轮共识。这导致在[核心协议](http://en.wikipedia.org/wiki/Paxos_algorithm)上开发了许多扩展，任何有兴趣构建基于Paxos系统的人仍然需要消化这些扩展。此外，还有更多的实际挑战，比如如何容易地使集群成员更改。

*ZAB*. ZAB - Zookeeper 原子广播协议，在Apache Zookeeper中使用。Zookeeper是一个为分布式系统提供协调服务的系统，很多以Hadoop为中心的分布式系统都使用它来进行协调（比如[HBase](http://hbase.apache.org/)、[Storm](http://storm-project.net/)、[Kafka](http://kafka.apache.org/)）。Zookeeper基本上是开源社区的Chubby版本。从技术上讲原子广播是一个不同于纯共识的问题，但它仍然属于分区容错算法的范畴，可以保证强一致性。

*Raft*. Raft是最近（2013年）新增的算法家族。它被设计成比Paxos更容易教学，同时提供同样的保证。特别是，算法的不同部分被更清楚地分开，论文还描述了一种集群成员变化的机制。最近受ZooKeeper的启发，它在[etcd](https://github.com/coreos/etcd)中得到了采用。

## 强一致性复制方法

在本章中，我们考察了强一致性的复制方法。从同步工作和异步工作之间的对比开始，我们一直到容错越来越复杂的故障算法。下面是每种算法的一些关键特征：

#### Primary/Backup

- 单点, 静态master
- 复制日志，从节点并不参与执行具体操作
- 复制延迟没有限制
- 没有分区容错
- r手动故障切换，没有容错, "热备份"

#### 2PC

- 一致表决：提交或终止
- 静态master
- 在提交期间，协调者和节点同时发生故障，2PC将无法起效
- 没有分区容错, 尾部延迟敏感

#### Paxos

- 多数表决
- 动态master
- 允许最多 n/2-1 个节点失效
- 对尾部延迟不太敏感

------

## 扩展阅读

#### 主从备份 与 2PC

- [Replication techniques for availability](http://scholar.google.com/scholar?q=Replication+techniques+for+availability) - Robbert van Renesse & Rachid Guerraoui, 2010
- [Concurrency Control and Recovery in Database Systems](http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx)

#### Paxos

- [The Part-Time Parliament](http://research.microsoft.com/users/lamport/pubs/lamport-paxos.pdf) - Leslie Lamport
- [Paxos Made Simple](http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf) - Leslie Lamport, 2001
- [Paxos Made Live - An Engineering Perspective](http://research.google.com/archive/paxos_made_live.html) - Chandra et al
- [Paxos Made Practical](http://scholar.google.com/scholar?q=Paxos+Made+Practical) - Mazieres, 2007
- [Revisiting the Paxos Algorithm](http://groups.csail.mit.edu/tds/paxos.html) - Lynch et al
- [How to build a highly available system with consensus](http://research.microsoft.com/lampson/58-Consensus/Acrobat.pdf) - Butler Lampson
- [Reconfiguring a State Machine](http://research.microsoft.com/en-us/um/people/lamport/pubs/reconfiguration-tutorial.pdf) - Lamport et al - changing cluster membership
- [Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4762) - Fred Schneider

#### Raft 与 ZAB

- [In Search of an Understandable Consensus Algorithm](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf), Diego Ongaro, John Ousterhout, 2013
- [Raft Lecture - User Study](http://www.youtube.com/watch?v=YbZ3zDzDnrw)
- [A simple totally ordered broadcast protocol](http://labs.yahoo.com/publication/a-simple-totally-ordered-broadcast-protocol/) - Junqueira, Reed, 2008
- [ZooKeeper Atomic Broadcast](http://labs.yahoo.com/publication/zab-high-performance-broadcast-for-primary-backup-systems/) - Reed, 2011